#### Adept Overview

Introduction:
	The idea should be fairly simple: use a program to generate potential/likely graspable approaches to an object, evaluate their effectiveness, and then execute the ones which look most promising. We are plumbers, as Ravi mentioned earlier; much of the system has aready been generously writen by others. We are responsible for hooking it up, refining it, and tuning it precisely to our needs.
	OpenRAVE (our grasp generator and evaluator) existed prior to the conception of this project. Qhull (our convex hull generator) existed before the DRC. People have been hacking the kinect as long as it has existed (freenect_launch and ar_track_alvar; our calibrated/reoriented pointcloud sources). And, last but not least, Moveit (our motion planner and collision checker) is as old as ROS.
	The above was a brief overview of the tools we use. To effectively use them, Forrest and myself have written interfaces between them and to existing libraries to perform additional operations which were not natively supported (that we know of). This includes pointcloud selection (filtering a 30cmx 30cm x 30cm box based on the point you click in Rviz; yeah, that's what you're doing to start the pipeline), object segmentation (picking the block of wood apart from the table; still a work in progress), custom grasp evaluation (favoring palm proximity to the object, reachability checking), and pregrasp pose functionality.
	Given the fact that we are building the system to two robots, we have needed to make things interchangeable and easily configurable. For this, we load specific parameters to ROS's built-in information server called the parameter server (look it up if you don't know). So, as the system runs and reaches points where Atlas behavior diverges from Adept behavior, it queries the parameter server for instructions. You can view these settings by opening hullify/launch/kinect_params.launch; you can change them too, but use rosparam, DO NOT MODIFY THEM IN THE TEXT FILE UNLESS YOU'VE CONSUTED JACKSON.

	So that's the very very brief overview of our tools and the pipeline.

The Pipeline: A paragraph per piece
	While the pipeline is running, view the system with: rosrun rqt_graph rqt_graph
	Everything starts in the cloud... well, in one of the clouds... from the kinect. Once connection has been established to the device, the clouds are pumped out at 30hz in full color and in 3d space, as you've seen. The clouds are published relative to the kinect's rgb frame (rostopic echo the pointcloud topics to see this). Calibration (ar_track_alvar) is responsible for transforming these points to the main, common reference frame (/world) so that the robot can understand where they are.
	The points are shown in Rviz, where an operator picks one and triggers the pipeline. This selected point is stored for later object segmentation (see below), but is immediately used to select a 30cm x 30cm x 30cm box of the entire cloud with that point as its center. This is the job of the ocs_listener node. The results of this selection are then piped to a transformer (selected_points_transformer), which changes them to the /world reference frame and gives them to the core pipeline.
	ALL the above is started by the command "roslaunch hullify kinect.launch"
	The first (of only two; don't worry, I know it's long already) stage in the pipeline takes a raw pointcloud possiby with other objects in the scene and attemts to clean it: anything that looks too large and table-like to grip is removed and the remaining groups of pointclouds are clustered
